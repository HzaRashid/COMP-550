\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\title{Reading Assignment 1: COMP 550, Fall 2024}
\author{Hamza Rashid}
\date{}
\begin{document}
\maketitle
\section*{Introduction}
% - Briefly introduce the paper "Neural Architectures for Named Entity Recognition" by Lample et al. (2016).
% - State that the paper presents two neural architectures for NER: bidirectional LSTM-CRF and transition-based chunking using stack-LSTMs.
% - Mention the main objective: developing language-agnostic NER systems without reliance on hand-crafted features or gazetteers.


The paper "Neural Architectures for 
Named Entity Recognition" by Lample et al. (2016)., 
addressed the reliance of state-of-the-art 
Named Entity Recognition (NER) systems on hand-crafted 
features and domain-specific knowledge, a approach
deemed necessary at the time, given the small, 
supervised training corpora that was available. 
The authors proposed two LSTM-based neural 
architectures designed to generalize without 
relying on external resources. Key 
components of their methodology included: 
IOBES tagging scheme (Inside, Outside, Beginning, 
End, Singleton), and forming character-sensitive word 
embeddings to capture orthographical and morphological details.

% evidence that
% a token, or sequential group of tokens, is a name.
% the dependence of classes on orthographic evidence,
% While the authors
% sucessfully acheive state-of-the-art performance,
% their methodology is also limited
% Shared at the input layer of these models 
% are embedding decisions aimed to capture two intuitions: 
% (i) reasoning jointly over tagging decisions as names 
% can span multiple tokens, 
% and (ii)


\section*{Paper Content Overview}

% a bidirectional LSTM supplemented with a CRF layer, and a greedy stack-based LSTM 
% that constructs and labels segments using a transition-based
% approach inspired by shift-reduce parsers

- Describe the key approaches used:
  - Bidirectional LSTM-CRF model.
  - Stack-LSTM-based transition model.
- Discuss the use of character-based word representations combined with unsupervised pre-trained embeddings to improve generalization.
- Mention the experiments conducted on English, Dutch, German, and Spanish datasets, and their state-of-the-art results.

\section*{Strengths and Limitations}
The researchers acheived the original task, 
- Strengths:
  - Language-independent, does not rely on hand-crafted features or gazetteers.
  - Achieves state-of-the-art results in multiple languages.
  - Effective use of character-based and word embeddings to handle morphology.
- Limitations:
  - The transition-based chunking model is more dependent on character-based information compared to the LSTM-CRF.
  - Greedy action selection in the Stack-LSTM model can lead to suboptimal results.
  The paper includes a detailed outline
  of the methodologies, and provides strong 
  justifications for its preprocessing decisions, 
  particularly at the input layer (arguing that LSTM's 
  are an a priori better function class for modeling 
  the relationship between words and their characters, 
  as they take into account position-variant features) .
\section*{Bidirectional LSTM-CRF Architecture (Figure 1)}
- Describe the key components:
  - Bidirectional LSTM: Encodes contextual information from both left and right contexts.
  - Conditional Random Field (CRF): Models dependencies between tags to produce globally optimal sequences.
- Explain the importance of these components:
  - Bidirectional LSTM captures comprehensive context for each word.
  - CRF layer ensures valid and coherent tag sequences.

\section*{Handling of OOV Items}
- Describe how the proposed method addresses out-of-vocabulary (OOV) words:
  - Uses character-level embeddings generated by a bidirectional LSTM to represent words based on their characters.
  - Incorporates pre-trained embeddings to handle unseen words by mapping them to a common UNK embedding during training.
- Compare to class discussions:
  - Similar to character-level models we discussed, which also leverage character features to address OOV problems.
  - Pre-trained embeddings are akin to word2vec embeddings we discussed for capturing distributional semantics.

\section*{Use of Gazetteers (Table 1)}
- Discuss methods incorporating gazetteers to improve NER performance:
  - Gazetteers can provide explicit, domain-specific named entity information, helping models generalize better.
  - Advantages: Improves recognition accuracy for specific entity types, especially in low-resource settings.
  - Disadvantages: Dependence on domain-specific resources reduces language independence and increases cost for new domains or languages.

\section*{Conclusion}
- Summarize the overall contribution of the paper.
- Highlight the effectiveness of neural architectures for NER without relying on language-specific features.

\end{document}