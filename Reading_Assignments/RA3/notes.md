1. What are the major differences in the pre-training of mBART vs. mBERT (or XLMR) in terms of architecture, pre-training task/noise function, and pre-training data?


2. Why did the authors learn a tokenizer on more languages than the languages covered in mBART25? Do you think this is a good idea? Discuss.

3. Do high-resource language pairs benefit from mBART pre-training at the fine-tuning stage? Contrast with low-resource language pairs and discuss.